{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.vision.face.models import TrainingStatusType, Person, SnapshotObjectType, OperationStatusType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: azure-cognitiveservices-vision-face in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied, skipping upgrade: msrest>=0.5.0 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-face) (0.6.10)\n",
      "Requirement already satisfied, skipping upgrade: azure-common~=1.1 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from azure-cognitiveservices-vision-face) (1.1.23)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: requests~=2.16 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: isodate>=0.6.0 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.5.0 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\tamittal\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade azure-cognitiveservices-vision-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tamittal\\\\source\\\\Untitled Folder'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://facedetectioncapstone.cognitiveservices.azure.com/'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['FACE_ENDPOINT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the FACE_SUBSCRIPTION_KEY environment variable with your key as the value.\n",
    "# This key will serve all examples in this document.\n",
    "KEY = os.environ['FACE_SUBSCRIPTION_KEY']\n",
    "#3e57e8993cfb4888863a13cf0d48cf85\n",
    "\n",
    "\n",
    "# Set the FACE_ENDPOINT environment variable with the endpoint from your Face service in Azure.\n",
    "# This endpoint will be used in all examples in this quickstart.\n",
    "ENDPOINT = os.environ['FACE_ENDPOINT']\n",
    "#https://facedetectioncapstone.cognitiveservices.azure.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an authenticated FaceClient.\n",
    "face_client = FaceClient(ENDPOINT, CognitiveServicesCredentials(KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in the Person Group Operations,  Snapshot Operations, and Delete Person Group examples.\n",
    "# You can call list_person_groups to print a list of preexisting PersonGroups.\n",
    "# SOURCE_PERSON_GROUP_ID should be all lowercase and alphanumeric. For example, 'mygroupname' (dashes are OK).\n",
    "PERSON_GROUP_ID = 'family'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person group: family\n",
      "{'additional_properties': {}, 'name': None, 'user_data': None, 'person_id': '2a785130-ffaf-453e-bafe-856c4f0e977d', 'persisted_face_ids': None}\n"
     ]
    }
   ],
   "source": [
    "#Create Person Group\n",
    "print('Person group:', PERSON_GROUP_ID)\n",
    "face_client.person_group.create(person_group_id=PERSON_GROUP_ID, name=PERSON_GROUP_ID)\n",
    "\n",
    "# Create Person in taht group\n",
    "bean = face_client.person_group_person.create(PERSON_GROUP_ID, \"bean\")\n",
    "shah = face_client.person_group_person.create(PERSON_GROUP_ID, \"shah\")\n",
    "print(bean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_images = [file for file in glob.glob('*.jfif') if file.startswith(\"download\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['download (1).jfif', 'download (2).jfif', 'download.jfif']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bean_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azure.cognitiveservices.vision.face._face_client.FaceClient object at 0x000001F7A02E2908>\n",
      "<azure.cognitiveservices.vision.face._face_client.FaceClient object at 0x000001F7A02E2908>\n",
      "<azure.cognitiveservices.vision.face._face_client.FaceClient object at 0x000001F7A02E2908>\n"
     ]
    }
   ],
   "source": [
    "for image in bean_images:\n",
    "    b = open(image, 'r+b')\n",
    "    face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, bean.person_id, b)\n",
    "    print(face_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "shah_images = [file for file in glob.glob('*.jfif') if file.startswith(\"shah\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shah1.jfif', 'shah2.jfif', 'shah3.jfif']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shah_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azure.cognitiveservices.vision.face._face_client.FaceClient object at 0x000001F7A02E2908>\n",
      "<azure.cognitiveservices.vision.face._face_client.FaceClient object at 0x000001F7A02E2908>\n",
      "<azure.cognitiveservices.vision.face._face_client.FaceClient object at 0x000001F7A02E2908>\n"
     ]
    }
   ],
   "source": [
    "for image in shah_images:\n",
    "    s = open(image, 'r+b')\n",
    "    face_client.person_group_person.add_face_from_stream(PERSON_GROUP_ID, shah.person_id, s)\n",
    "    print(face_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the person group...\n",
      "Training status: running.\n",
      "\n",
      "Training status: succeeded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train PersonGroup\n",
    "'''\n",
    "print()\n",
    "print('Training the person group...')\n",
    "# Train the person group\n",
    "face_client.person_group.train(PERSON_GROUP_ID)\n",
    "\n",
    "while (True):\n",
    "    training_status = face_client.person_group.get_training_status(PERSON_GROUP_ID)\n",
    "    print(\"Training status: {}.\".format(training_status.status))\n",
    "    print()\n",
    "    if (training_status.status is TrainingStatusType.succeeded):\n",
    "        break\n",
    "    elif (training_status.status is TrainingStatusType.failed):\n",
    "        sys.exit('Training the person group has failed.')\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee0353b7-5371-4127-aaaa-f9264744aef6\n",
      "37ce841e-c478-43f5-a787-6fd425056b87\n",
      "4b7523e5-2a04-4e72-a291-71d370d3a5ef\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Identify a face against a defined PersonGroup\n",
    "'''\n",
    "# Group image for testing against\n",
    "group_photo = 'srk.jfif'\n",
    "IMAGES_FOLDER=os.getcwd()\n",
    "#IMAGES_FOLDER = os.path.join(os.path.dirname(os.path.realpath(__file__)))\n",
    "# Get test image\n",
    "test_image_array = glob.glob(os.path.join(IMAGES_FOLDER, group_photo))\n",
    "image = open(test_image_array[0], 'r+b')\n",
    "#print(image)\n",
    "# Detect faces\n",
    "face_ids = []\n",
    "faces = face_client.face.detect_with_stream(image)\n",
    "#print(faces)\n",
    "for face in faces:\n",
    "    print(face.face_id)\n",
    "    face_ids.append(face.face_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_properties': {}, 'face_id': '37ce841e-c478-43f5-a787-6fd425056b87', 'candidates': []}\n",
      "Identifying faces in srk.jfif\n",
      "Can not identify this person for face ID ee0353b7-5371-4127-aaaa-f9264744aef6\n",
      "Can not identify this person for face ID 37ce841e-c478-43f5-a787-6fd425056b87\n",
      "Person for face ID 4b7523e5-2a04-4e72-a291-71d370d3a5ef is identified in srk.jfif with a confidence of 0.7569.\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count=0\n",
    "# Identify faces\n",
    "#print(face_ids, PERSON_GROUP_ID)\n",
    "results = face_client.face.identify(face_ids, PERSON_GROUP_ID)\n",
    "print(results[1])\n",
    "print('Identifying faces in {}'.format(os.path.basename(image.name)))\n",
    "if not results:\n",
    "    print('No person identified in the person group for faces from {}.'.format(os.path.basename(image.name)))\n",
    "for person in results: \n",
    "    if (person.candidates== []) :\n",
    "        print(\"Can not identify this person for face ID {}\".format(person.face_id))\n",
    "        count=count+10\n",
    "        continue;\n",
    "    #print(person)\n",
    "    #print(person.face_id)\n",
    "    #print(os.path.basename(image.name))\n",
    "    #print(person)\n",
    "    print('Person for face ID {} is identified in {} with a confidence of {}.'.format(person.face_id, os.path.basename(image.name), person.candidates[0].confidence)) # Get topmost confidence score\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
